{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 5: Feature Engineering and Machine Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Data Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the data set and run the Logistic Regression (LGR) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple\n",
    "from IPython.display import display\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# ignoring the annoying warning messages (e.g., from LGR model)\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_tracker():\n",
    "    \"\"\"\n",
    "    Generator function to track accuracy scores for different models\n",
    "    \"\"\"\n",
    "    # Initialize an empty dataframe to store the results\n",
    "    results = pd.DataFrame(columns=[\"Model Name\", \"Accuracy Score\"])\n",
    "    \n",
    "    while True:\n",
    "\n",
    "        # Receive a tuple with the model name and accuracy score\n",
    "        model_name, accuracy = (yield)\n",
    "        \n",
    "        # Append the received data to the results dataframe\n",
    "        new_result = pd.DataFrame({\n",
    "            \"Model Name\": [model_name],\n",
    "            \"Accuracy Score\": [accuracy]\n",
    "        })\n",
    "        results = pd.concat([results, new_result], ignore_index=True)\n",
    "        display(results)\n",
    "\n",
    "# Initialize the generator function\n",
    "tracker = accuracy_tracker()\n",
    "\n",
    "# Prime the generator function to start tracking accuracy scores\n",
    "next(tracker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset into a pandas dataframe\n",
    "df = pd.read_csv('winequality-red.csv')\n",
    "\n",
    "def LGR(data: pd.DataFrame, target: str = 'quality') -> Tuple[float, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Logistic Regression Model (LGR) function to \n",
    "    - split data into features and target (`quality`)\n",
    "    - train LGR classifier\n",
    "    - make predictions on the test set\n",
    "    - return accuracy score (float) and feature weights (dataframe)\n",
    "    \"\"\"\n",
    "    # Split the dataset into features and target\n",
    "    X = data.drop(target, axis=1)\n",
    "    y = data[target]\n",
    "\n",
    "    #convert y values to categorical values\n",
    "    y_transformed = LabelEncoder().fit_transform(y)\n",
    "\n",
    "    # Split the dataset into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_transformed, test_size=0.2, random_state=0)\n",
    "\n",
    "    # Train a logistic regression classifier on the training data\n",
    "    clf = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # ----- ACCURACY SCORE -----\n",
    "    # Make predictions on the test data\n",
    "    y_pred = clf.predict(X_test)\n",
    "    # Assess the accuracy\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # ----- FEATURE WEIGHTS -----\n",
    "    # Get the feature coefs (importance)\n",
    "    coefs = clf.coef_[0]\n",
    "    # Get the column names of the features\n",
    "    features = X.columns\n",
    "    # Combine the feature names and coefficients into a DataFrame\n",
    "    df_coefs = pd.DataFrame({'Feature': features, 'Coef': coefs})\n",
    "    # Create a new column based on the abs. val. of the coefs (i.e., ignoring directionality)\n",
    "    df_coefs['Magnitude'] = abs(df_coefs['Coef'])\n",
    "    # Sort the DataFrame by magnitudes\n",
    "    df_weights = df_coefs.sort_values(by='Magnitude', ascending=False)\n",
    "\n",
    "    # Return the accuracy score and feature weights\n",
    "    return acc, df_weights\n",
    "\n",
    "acc, _ = LGR(df)\n",
    "tracker.send((\"Model 1: Default Data (LGR)\", acc))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An accuracy score of 0.61875 is relatively low; the logistic regression classifier does not perform well on the unprocessed wine quality dataset. It may be necessary to process the data further such as by normalizing the features, imputing missing values, or removing outliers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Explore and sanitize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_more(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Helper function that returns extended descriptive statistics on missingness for a Pandas data frame\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get basic descriptive statistics for the data using the describe() method\n",
    "    describe = data.describe().T\n",
    "    \n",
    "    # Drop the 25%, 50%, and 75% quantiles from the basic statistics\n",
    "    less = describe.drop(columns=['25%', '50%', '75%'])\n",
    "    \n",
    "    # Add the number and percentage of missing values, and the data type for each column\n",
    "    more = less.assign(num_missing = data.isna().sum(),\n",
    "                       perc_missing = round(data.isna().mean() * 100, 2),\n",
    "                       dtype = data.dtypes)\n",
    "    \n",
    "    return more\n",
    "\n",
    "# Apply the describe_more function to the wine quality data set\n",
    "describe_more(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a box plot of the unedited dataset using Plotly Express\n",
    "fig = px.box(df, boxmode='group', orientation='h')\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several features in the wine quality dataset have a large number of outliers and are not normally distributed, including `total sulfur dioxide`, `free sulfur dioxide`, `residual sugar`, and `fixed acidity`. Although these outliers may represent genuine values in the context of red wines, they can greatly influence the predictive capabilities of models and result in overfitting or underfitting. To address this, I will remove the outlier values by trimming them. Winsorizing, where outliers are imputed to reduce their impact, is another method to consider in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_outliers(data: pd.Series, factor: float=1.5) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Helper function to trim outliers from a panda series using the Tukey method\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the first (Q1) and third (Q3) quartiles of the data\n",
    "    Q1, Q3 = np.percentile(data, 25), np.percentile(data, 75)\n",
    "    \n",
    "    # Calculate the interquartile range (IQR)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    # Calculate the lower and upper fences using the factor and IQR\n",
    "    lower_fence, upper_fence = (Q1 - factor * IQR), (Q3 + factor * IQR)\n",
    "    \n",
    "    # Return the data that falls within the lower and upper fences\n",
    "    return data[(data >= lower_fence) & (data <= upper_fence)]\n",
    "\n",
    "# Trim outliers from each column (except `quality`) of the data set using the trim_outliers function\n",
    "df_trimmed = df.drop(\"quality\", axis=1).apply(lambda x: trim_outliers(x))\n",
    "df_trimmed[\"quality\"] = df[\"quality\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trimming the data results in missing values as the outliers for each feature are forcibly removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the trimmed data and sort the result by percentage of missing values in descending order\n",
    "describe_more(df_trimmed).sort_values(by='perc_missing', ascending=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most machine learning models do not handle missing data well. There are several common approaches to addressing missing values:\n",
    "\n",
    "- Dropping rows, which is typically suitable when the number of missing values is less than or equal to 2-3% of the total number of rows\n",
    "- Imputing missing data by replacing it with the mean, median, or mode value of the column\n",
    "- Predicting missing data using statistical or machine learning models, which is considered a more sophisticated and accurate approach.\n",
    "\n",
    "For this dataset, I will use Sklearn's experimental `IterativeImputer`, which is a predictive method for handling missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an IterativeImputer object with a maximum of 10 iterations and a random state of 0\n",
    "imputer = IterativeImputer(max_iter=10, random_state=0)\n",
    "\n",
    "# Fit the imputer to the trimmed data and transform the data to impute missing values\n",
    "df_imputed = imputer.fit_transform(df_trimmed)\n",
    "\n",
    "# Convert the imputed data to a Pandas data frame with the same column names as the trimmed data\n",
    "df_imputed = pd.DataFrame(df_imputed, columns=df_trimmed.columns)\n",
    "\n",
    "# Describe the imputed data --> no more missing values!\n",
    "describe_more(df_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a box plot of the imputed data using Plotly Express --> looking much better than before!\n",
    "fig = px.box(df_imputed, boxmode='group', orientation='h')\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Examine pair-wise correlations between all features and the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_last_to_first(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Helper function to re-order last column to first position in a dataframe\n",
    "    \"\"\"\n",
    "    # Get the list of column names and move the last column to the front of the list\n",
    "    cols = data.columns.tolist()\n",
    "    cols = cols[-1:] + cols[:-1]\n",
    "\n",
    "    # Return the re-ordered columns in the dataframe using the new order of column names\n",
    "    return data[cols]\n",
    "\n",
    "df_imputed = move_last_to_first(df_imputed)\n",
    "\n",
    "sns.set_theme(style=\"white\")\n",
    "\n",
    "# Compute the pairwise correlations between features in the dataframe\n",
    "corr = df_imputed.corr()\n",
    "\n",
    "# Mask the lower triangle of the correlation matrix to exclude redundant values\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Plot the correlation matrix heatmap\n",
    "sns.heatmap(corr, annot=True, mask=mask, cmap='RdBu', center=0, annot_kws={\"size\": 8})\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features most correlated with the target variable include `alchohol` (+0.48), `sulphates` (+0.40), and `volatile acidity` (-0.37)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Initial Model Run, Feature Engineering, and Model Parameter Tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Running the Logistic Regression (LGR) model on sanitized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, _ = LGR(df_imputed)\n",
    "tracker.send((\"Model 2: Sanitized Data (LGR)\", acc))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Feature Engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As each feature corresponds with a different scale, it is necessary to normalize features so they fall within the same range.\n",
    "\n",
    "Scikit-learn docs state that `StandardScaler` \"might behave badly if the individual features do not more or less look like standard normally distributed data.\" The histograms below reveal that several features are non-normally distributed.\n",
    "\n",
    "On the other hand, `MinMaxScale` will preserve the shape of the data while transforming values between 0 and 1. I am opting for this latter approach as it should perform better than `StandardScaler` for non-normally distributed features. That said, it should be noted that both `StandardScaler` and `MinMaxScaler` are very sensitive to the presence of outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed.hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the imputed dataset with MinMaxScaler\n",
    "df_normalized = scaler.fit_transform(df_imputed)\n",
    "\n",
    "# Convert normalized data back into a pandas dataframe with original column names\n",
    "df_normalized = pd.DataFrame(df_normalized, columns=df_imputed.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, _ = LGR(df_normalized)\n",
    "tracker.send((\"Model 3: Normalized Data (LGR)\", acc))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, accuracy decreased after normalizing.\n",
    "\n",
    "This holds true for `StandardScaler`, `RobustScaler`, and `MinMaxScaler` (tested them all - but used the latter here), which leads me to believe that 1) there might a better, non-linear normalization method, 2) normalizing generally impacts accuracy negatively, or 3) the randomness involved with train/test splits just happened to hurt the accuracy in this case."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Binarizing the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the threshold\n",
    "threshold = 5\n",
    "\n",
    "# Copy df_imputed (pre-normalization) to df_binned, and create a new column called quality_bin, set to 0\n",
    "df_binned = df_imputed\n",
    "\n",
    "# Set the value of quality_bin to 1 if quality is greater than or equal to the threshold, otherwise 0\n",
    "df_binned['quality_bin'] = (df_imputed['quality'] >= threshold).astype(int)\n",
    "df_binned = move_last_to_first(df_binned)\n",
    "\n",
    "# Drop the `quality` column, otherwise LGR model will learn this feature\n",
    "df_binned = df_binned.drop('quality', axis=1)\n",
    "\n",
    "acc, _ = LGR(df_binned, target='quality_bin')\n",
    "print(f\"accuracy when threshold=={threshold}: {acc}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is suddenly extremely high!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_binned['quality_bin'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of a problem called class imbalance, where one class dominates the training data and the model becomes biased towards predicting that class."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Trying different threshold values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store the accuracy scores and threshold values\n",
    "accuracies = []\n",
    "thresholds = []\n",
    "model_num = 4\n",
    "\n",
    "# Loop through different threshold values, from 4 to 8\n",
    "for threshold in range(4, 9):\n",
    "    df_binned = df_imputed\n",
    "    df_binned['quality_bin'] = (df_imputed['quality'] >= threshold).astype(int)\n",
    "    df_binned = move_last_to_first(df_binned)\n",
    "\n",
    "    # Drop the `quality` column, otherwise LGR model will learn this feature\n",
    "    df_binned = df_binned.drop('quality', axis=1)\n",
    "\n",
    "    print(f'Threshold value == {threshold}')\n",
    "    print(df_binned['quality_bin'].value_counts())\n",
    "\n",
    "    # Evaluate the model on the testing set\n",
    "    acc, _ = LGR(df_binned, target='quality_bin')\n",
    "    accuracies.append(acc)\n",
    "    thresholds.append(threshold)\n",
    "    \n",
    "    tracker.send((f\"Model {model_num}: Binned Data threshold={threshold} (LGR)\", acc))\n",
    "    model_num += 1\n",
    "    print('\\n')\n",
    "\n",
    "# Plot the results\n",
    "plt.scatter(thresholds, accuracies)\n",
    "plt.plot(thresholds, accuracies)\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.title('Accuracy Scores by Threshold Value')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, accuracy is highest when there is a large class imbalance (not a good thing). Accuracy drops when classes are balanced.\n",
    "\n",
    "A threshold of `6` seems ideal, as classes are evenly balanced and accuracy has improved in comparison to Models 1, 2, and 3.\n",
    "\n",
    "That said, oenologists and wine connoisseurs might prefer highly specific predictions of quality (as done prior to binarizing), whereas laypersons might be satisfied with less specific predictions (i.e., good versus bad). Thus, it's important to consider users when deciding on whether to bin/binarize the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the ideal number of bins into `df_ideal_bins` for later use:\n",
    "ideal_bins = 6\n",
    "df_ideal_bins = df_imputed\n",
    "df_ideal_bins['quality_bin'] = (df_imputed['quality'] >= ideal_bins).astype(int)\n",
    "df_ideal_bins = move_last_to_first(df_ideal_bins)\n",
    "df_ideal_bins = df_ideal_bins.drop('quality', axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Which of the features are most important in predicting quality?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As coefficients are hard to interpret if features are on different scales, I will opt to use `df_normalized` to weigh the relative importance of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, weights = LGR(df_normalized)\n",
    "display(weights)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the correlation matrix heatmap above had found `alcohol` and `sulphates` to be the most correlated with the target variable, the coefficients of the logistic regression model reveal that `volatile acidity`, `citric acid`, and `total sulfur dioxide` impact the predictions the most in terms of the magnitudes of their coefficients."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Other Algorithms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Considering another algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient boosting machine (GBM) might be a good option, as `quality` is categorical and decision trees/random forests/GBMs are good options for predicting categorical target variable(s). Unlike a simple decision tree, GBMs might be less prone to overfitting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Model hyper-parameters of this algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `n_estimators`: Number of trees in the forest. The greater the number of trees, the more accurate the model will be, but this also increases the risk of overfitting.\n",
    "\n",
    "- `max_depth`: Maximum depth of each tree in the forest. A tree with a larger maximum depth will be able to model more complex relationships in the data, but it also increases the risk of overfitting.\n",
    "\n",
    "- `min_samples_split`: Minimum number of samples required to split an internal node in the tree. Setting this to a higher value can help prevent overfitting by reducing the size of each tree.\n",
    "\n",
    "- `learning_rate`: Contribution of each tree in the final prediction. A smaller learning rate means that each tree contributes less, so more trees are needed to achieve the same overall accuracy.\n",
    "\n",
    "- `loss`: Loss function to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def GBM(data: pd.DataFrame, target: str = 'quality') -> float:\n",
    "    \"\"\"\n",
    "    Gradient Boosting Machine (GBM) function to \n",
    "    - split data into features and target (`quality`)\n",
    "    - train GBM classifier\n",
    "    - make predictions on the test set\n",
    "    - return accuracy score (float)\n",
    "    \"\"\"\n",
    "    # Split the dataset into features and target\n",
    "    X = data.drop(target, axis=1)\n",
    "    y = data[target]\n",
    "\n",
    "    #convert y values to categorical values\n",
    "    y_transformed = LabelEncoder().fit_transform(y)\n",
    "\n",
    "    # Split the dataset into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_transformed, test_size=0.2, random_state=0)\n",
    "\n",
    "    # Params deriven by GridSearchCV to test a range of values for each hyper-parameter\n",
    "    params = {\n",
    "        'learning_rate': 0.1,   # GridSearchCV: [0.1, 0.05, 0.01]\n",
    "        'max_depth': 7,         # GridSearchCV: [3, 5, 7]\n",
    "        'n_estimators': 100     # GridSearchCV: [100, 500, 1000]\n",
    "    }\n",
    "\n",
    "    # Train a GBM classifier on the training data\n",
    "    clf = GradientBoostingClassifier(**params)\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Return the accuracy score\n",
    "    return accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Re-running another algorithm with the original, sanitized, normalized, and binned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize another instance of the generator function for GBM accuracies\n",
    "tracker_GBM = accuracy_tracker()\n",
    "next(tracker_GBM)\n",
    "\n",
    "tracker_GBM.send((\"Model 1: Default Data (GBM)\", GBM(df)))\n",
    "tracker_GBM.send((\"Model 2: Sanitized Data (GBM)\", GBM(df_imputed)))\n",
    "tracker_GBM.send((\"Model 3: Normalized Data (GBM)\", GBM(df_normalized)))\n",
    "tracker_GBM.send((\"Model 3: Binned Data threshold=6 (GBM)\", GBM(df_ideal_bins, target='quality_bin')))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to LGR models, the sanitized data and binned data of GBM models had the highest accuracies. Surprisingly, the sanitized data had a remarkably higher accuracy than other models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Which is more robust?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Across the board, the GBM models performed better than the LGR models in terms of accuracy! This might be because:\n",
    "\n",
    "- **Non-Linearity**: GBM might capture non-linear relationships better than LGR\n",
    "- **Feature Interactions**: GBM can captures interactions between features, which LGR cannot do\n",
    "- **Noise**: GBMs can handle noisy or irrelevant features by assigning lower weights to them\n",
    "- **Imbalances**: GBMs can handle imbalanced classes better than LGR"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
